# Структура папок и назначение примеров

В этой директории представлены примеры, демонстрирующие, как Triton Inference Server может взаимодействовать с различными форматами моделей и бэкендами. Каждый пример предназначен для иллюстрации развертывания и использования моделей в различных сценариях.

## Назначение примеров

- **[ONNX](./ONNX)**: Демонстрирует, как развернуть модель в формате ONNX на Triton Inference Server. ONNX позволяет обмениваться моделями между различными фреймворками, обеспечивая совместимость.

- **[OpenVINO](./OpenVino)**: Показывает, как использовать OpenVINO для оптимизации и развертывания моделей, что позволяет ускорять инференс на различных аппаратных платформах.

- **[TensorFlow](./TensorFlow)**: Пример развертывания модели TensorFlow в формате SavedModel. Это демонстрирует, как Triton может использовать популярные фреймворки для инференса.

- **[Python](./Python)**: Пример использования Python Backend для выполнения пользовательской логики, такой как простая арифметическая операция. Это позволяет интегрировать пользовательские обработки данных и использовать библиотеки Python.

- **[PyTorch](./PyTorch)**: Демонстрирует, как развернуть модель PyTorch на Triton Inference Server. PyTorch широко используется для исследований и разработки в области глубокого обучения.

- **[TensorRT](./TensorRT)**: Пример использования TensorRT для оптимизации и развертывания моделей, что позволяет значительно ускорить инференс на GPU.

Эти примеры предназначены для демонстрации гибкости Triton Inference Server в работе с различными форматами моделей и бэкендами, а также для предоставления практических примеров развертывания и использования моделей в различных сценариях.