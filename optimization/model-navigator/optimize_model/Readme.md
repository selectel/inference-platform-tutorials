# Развертывание линейной модели на сервере Triton Inference

Этот пример демонстрирует, как оптимизировать простую линейную модель и развернуть её на сервере Triton Inference.

## Обзор Triton Model Navigator

Triton Model Navigator - инструмент для оптимизации и развертывания моделей глубокого обучения с акцентом на GPU от NVIDIA. Triton Model Navigator упрощает процесс переноса моделей и конвейеров, реализованных в PyTorch, TensorFlow и/или ONNX, в TensorRT.

Triton Model Navigator автоматизирует несколько критически важных шагов, включая экспорт модели, конвертацию, тестирование корректности и профилирование. Предоставляя единую точку входа для различных поддерживаемых фреймворков, пользователи могут эффективно искать лучший вариант развертывания, используя функцию оптимизации для каждого фреймворка. Полученные оптимизированные модели готовы к развертыванию как на PyTriton, так и на Triton Inference Server.

## Требования

Используйте контейнер NVIDIA Torch:
````shell
docker run -it --gpus 1 --shm-size 8gb -v ${PWD}:${PWD} -w ${PWD} nvcr.io/nvidia/pytorch:24.10-py3 bash
````

## Установка Model Navigator

Установите Triton Model Navigator, следуя руководству по установке для Torch:

````shell
pip install -U --extra-index-url https://pypi.ngc.nvidia.com triton-model-navigator[torch]
````

## Запуск оптимизации модели

На следующем этапе будет выполнен процесс оптимизации модели.

````bash
pip install --upgrade grpcio==1.68.0
python optimize.py
````

После завершения процесса в текущем рабочем каталоге будет создан каталог `model_repository`.
На этом этапе контейнер завершает работу.

````bash
exit
````

## Созданный артефакт в папке navigator_workspace

Структура папки `navigator_workspace` содержит различные артефакты, созданные в процессе оптимизации и развертывания модели. Вот описание содержимого:
# Структура папки navigator_workspace/06_optimize_and_serve_model_on_triton

- **context.yaml**: Файл конфигурации, содержащий контекст выполнения оптимизации.

- **model_input/**: Директория, содержащая входные данные для различных этапов оптимизации.
  - **conversion/**: Входные данные для этапа конверсии.
  - **correctness/**: Входные данные для тестирования корректности.
  - **profiler/**: Входные данные для профилировщика.
  - **profiling/**: Входные данные для этапа профилирования.

- **model_output/**: Директория, содержащая выходные данные для различных этапов оптимизации.
  - **conversion/**: Выходные данные для этапа конверсии.
  - **correctness/**: Выходные данные для тестирования корректности.
  - **profiling/**: Выходные данные для этапа профилирования.

- **navigator.log**: Лог-файл, содержащий подробную информацию о процессе оптимизации.

- **onnx/**: Директория, содержащая артефакты, связанные с форматом ONNX.
  - **format.log**: Лог-файл формата.
  - **model.onnx**: Экспортированная модель в формате ONNX.
  - **reproduce_*.py** и **reproduce_*.sh**: Скрипты для воспроизведения различных этапов (корректность, экспорт, профилирование).

- **optimize_report.txt**: Текстовый отчет об оптимизации, содержащий результаты и метрики.

- **status.yaml**: Файл статуса, содержащий информацию о текущем состоянии оптимизации.

- **torch/**: Директория, содержащая артефакты, связанные с форматом Torch.
  - **format.log**: Лог-файл формата.
  - **reproduce_*.py** и **reproduce_*.sh**: Скрипты для воспроизведения различных этапов (корректность, профилирование).

- **torch-exportedprogram/**: Директория для моделей, экспортированных программно в формате Torch.
  - **format.log**: Лог-файл формата.
  - **model.pt2**: Экспортированная модель.
  - **reproduce_*.py** и **reproduce_*.sh**: Скрипты для воспроизведения различных этапов.

- **torch-trt-fp16/** и **torch-trt-fp32/**: Директории для моделей, оптимизированных с использованием Torch-TensorRT в форматах FP16 и FP32.
  - **format.log**: Лог-файл формата.
  - **reproduce_conversion.py** и **reproduce_conversion.sh**: Скрипты для воспроизведения этапа конверсии.

- **torchscript-script/** и **torchscript-trace/**: Директории для моделей, оптимизированных с использованием TorchScript.
  - **format.log**: Лог-файл формата.
  - **model.pt**: Экспортированная модель.
  - **reproduce_*.py** и **reproduce_*.sh**: Скрипты для воспроизведения различных этапов.

- **trt-fp16/** и **trt-fp32/**: Директории для моделей, оптимизированных с использованием TensorRT в форматах FP16 и FP32.
  - **format.log**: Лог-файл формата.
  - **model.plan**: План модели в формате TensorRT.
  - **reproduce_*.py** и **reproduce_*.sh**: Скрипты для воспроизведения различных этапов.


## Запуск сервера Triton Inference

На основе созданного развертывания в репозитории моделей можно запустить сервер Triton Inference.
Следующая команда запускает сервер в фоновом режиме и открывает порты HTTP и gRPC.

````bash
docker run --gpus=1 --rm -d \
  --name tritonserver \
  -p8000:8000 \
  -p8001:8001 \
  -p8002:8002 \
  -v ${PWD}/model_repository:/models \
  nvcr.io/nvidia/tritonserver:24.10-py3 \
  tritonserver --model-repository=/models
````

## Использование Perf Analyzer для профилирования модели

Наконец, вы можете запустить контейнер с Perf Analyzer:
````shell
docker run -it --network=host nvcr.io/nvidia/tritonserver:24.10-py3-sdk bash
````

## Удаление контейнеров

После завершения выполнения примера удалите контейнер Triton, работающий в фоновом режиме:
````bash
docker stop tritonserver
````