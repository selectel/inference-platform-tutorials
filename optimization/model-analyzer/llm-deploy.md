# Развертывание VLLM с оптимальной конфигурацией triton

Этот пример демонстрирует, как подобрать конфигурацию triton для VLLM.
## Обзор Triton Model Analyzer

Triton Model Analyzer - 

Triton Model Navigator автоматизирует несколько критически важных шагов, включая экспорт модели, конвертацию, тестирование корректности и профилирование. Предоставляя единую точку входа для различных поддерживаемых фреймворков, пользователи могут эффективно искать лучший вариант развертывания, используя функцию оптимизации для каждого фреймворка. Полученные оптимизированные модели готовы к развертыванию как на PyTriton, так и на Triton Inference Server.

### Принцип работы:

Model Analyzer использует итеративный подход для поиска оптимальной конфигурации:

1. **Запуск экземпляров Triton:**
   - Для каждой тестируемой конфигурации запускается отдельный экземпляр Triton Inference Server
   - Каждый экземпляр получает свой набор параметров (размер батча, количество инстансов и т.д.)

2. **Профилирование производительности:**
   - Для каждой конфигурации запускается Perf Analyzer (perf_client)
   - Измеряются ключевые метрики: латентность, пропускная способность, использование GPU/CPU/памяти
   - Perf Analyzer генерирует синтетическую нагрузку с разным количеством параллельных запросов

3. **Анализ результатов:**
   - Собираются метрики со всех запущенных конфигураций
   - Отфильтровываются конфигурации, не соответствующие заданным ограничениям QoS
   - Выбирается оптимальная конфигурация на основе целевой функции (например, максимальная пропускная способность при заданной латентности)

4. **Генерация отчетов:**
   - Создаются детальные отчеты по каждой протестированной конфигурации
   - Формируются графики зависимостей между различными параметрами
   - Предоставляются рекомендации по оптимальной конфигурации

## Как происходит подбор конфигурации вручную
Сначала разберем как можно подбирать конфигурацию вручную.

### Шаг 1:
Используйте контейнер NVIDIA VLLM:
```shell
docker run -ti \
  --gpus all \
  --network=host \
  --shm-size=1g --ulimit memlock=-1 \
  -v ${HOME}/models:/root/models \
  -v ${HOME}/.cache/huggingface:/root/.cache/huggingface \
  nvcr.io/nvidia/tritonserver:25.01-vllm-python-py3
```

Установим [triton cli](https://github.com/triton-inference-server/triton_cli) для импорта модели VLLM, например qwen-1.5b. Утилита позволяет импортировать модель с дефолтным конфигурационным файлом.

```shell
pip install git+https://github.com/triton-inference-server/triton_cli.git
```

И импортируем deepseek на основе qwen1.5b
```shell
triton import --model deepseek-r1-distill-qwen-1.5b  --source "hf:deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
```
### Шаг 2:
Получим такую структуру и содержимое файлов:
```
/root/models/
└── deepseek-r1-distill-qwen-1.5b/
    ├── 1/
    │   └── model.json
    └── config.pbtxt

# model.json
{
  "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", 
  "disable_log_requests": true,
  "gpu_memory_utilization": 0.85
}
```
Также в самом конфигурационном файле `config.pbtxt` мы по-дефолту получаем следующие параметры:
```
# config.pbtxt
backend: "vllm"
instance_group [{kind: KIND_MODEL}]
```

### Шаг 3:
Скачаем веса, запустив модель
```shell
triton start --model-repository /root/models --model deepseek-r1-distill-qwen-1.5b
```
### Шаг 4:
Для подбора конфигурации, model-analyzer использует [genai-perf](https://github.com/triton-inference-server/perf_analyzer/blob/main/genai-perf/README.md) на основе [perf_client](https://github.com/triton-inference-server/perf_client).
Протестируем запрос

```shell
genai-perf profile -m deepseek-r1-distill-qwen-1.5b --service-kind triton --backend vllm --streaming
```

В результате теста должны получить следующую картинку
```
                              NVIDIA GenAI-Perf | LLM Metrics                              
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓
┃                         Statistic ┃    avg ┃    min ┃    max ┃    p99 ┃    p90 ┃    p75 ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩
│          Time to first token (ms) │  33.67 │  31.89 │ 305.72 │  35.12 │  33.80 │  32.77 │
│          Inter token latency (ms) │   4.82 │   3.10 │   6.84 │   5.70 │   5.35 │   5.08 │
│              Request latency (ms) │ 119.48 │ 117.33 │ 390.83 │ 120.63 │ 119.31 │ 118.68 │
│            Output sequence length │  18.98 │  16.00 │  29.00 │  26.00 │  21.00 │  20.00 │
│             Input sequence length │ 550.05 │ 550.00 │ 553.00 │ 551.00 │ 550.00 │ 550.00 │
│ Output token throughput (per sec) │ 158.67 │    N/A │    N/A │    N/A │    N/A │    N/A │
│      Request throughput (per sec) │   8.36 │    N/A │    N/A │    N/A │    N/A │    N/A │
└───────────────────────────────────┴────────┴────────┴────────┴────────┴────────┴────────┘
2025-02-05 16:05 [INFO] genai_perf.export_data.json_exporter:62 - Generating artifacts/deepseek-r1-distill-qwen-1.5b-triton-vllm-concurrency1/profile_export_genai_perf.json
2025-02-05 16:05 [INFO] genai_perf.export_data.csv_exporter:71 - Generating artifacts/deepseek-r1-distill-qwen-1.5b-triton-vllm-concurrency1/profile_export_genai_perf.csv
```

далее возвращаемся на шаг 2 и меняем конфигурационный файл `config.pbtxt`, а именно параметр `instance_group`, 'dynamic_batching', 'max_batch_size', 'max_queue_delay_microseconds'. Смотрим на результаты теста и повторяем процесс до тех пор, пока не получим оптимальную конфигурацию.


### Вывод:
Такой способ требует много времени, так как приходится конфигурировать тест каждый раз отдельно.

## Как происходит подбор конфигурации с помощью GenAI-Perf Analyze

Шаги 1-3 аналогичны предыдущему примеру.

## Шаг 4:
Используем режим analyze для запуска сразу нескольких тестов с разными параметрами.

В режиме analyze при запуске genai-perf можно указать параметры `--sweep-type` (какой параметр будем изменять) и `--sweep-range` (в каких пределах). Ниже описание каждого параметра и сценариев использования.

## Доступные параметры --sweep-type

### batch_size
**Что это такое:**
- Количество запросов, объединяемых в один батч при отправке к серверу

**Когда использовать:**
- Если система поддерживает или использует батчинг
- При инференсе в фреймворках, группирующих входные запросы

**Зачем:**
- Помогает понять масштабируемость модели при разном размере батча
- Обычно увеличенный batch_size повышает throughput, но может увеличивать latency

### concurrency
**Что это такое:**
- Количество параллельных запросов, отправляемых одновременно

**Когда использовать:**
- Практически всегда полезно для понимания способности серверной части обрабатывать нагрузку
- Когда нужно оценить масштабируемость по параллелизму

**Зачем:**
- Определение оптимального числа одновременных запросов
- Поиск баланса между максимальным throughput и приемлемым latency

### request_rate
**Что это такое:**
- Скорость генерации запросов (RPS — Requests Per Second)
- Генератор нагрузки поддерживает заданную частоту запросов

**Когда использовать:**
- При необходимости тестирования с постоянным потоком входящих запросов
- Для поиска точки насыщения сервера

**Зачем:**
- Определение максимальной пропускной способности при заданном SLA
- Выявление пороговых значений латентности и процента ошибок

### input_sequence_length
**Что это такое:**
- Размер входной последовательности (количество токенов в промпте)

**Когда использовать:**
- При необходимости проверки производительности с разной длиной входных данных
- Для тестирования как коротких, так и длинных промптов

**Зачем:**
- Анализ зависимости производительности от длины промпта
- Оптимизация настроек под разные типы входных данных

### num_dataset_entries
**Что это такое:**
- Количество записей в датасете промптов за один цикл тестирования

**Когда использовать:**
- При наличии большого набора различных промптов
- Для проверки влияния разнообразия входных данных

**Зачем:**
- Оценка производительности на разных объемах уникальных запросов
- Анализ масштабируемости при увеличении разнообразия входных данных


## Параметр --sweep-range
- Общее значение: позволяет указать диапазон и шаг (или степени двойки), по которому вы хотите прогнать тесты.

  - Формат min:max (без шага): тогда программа будет прогонять значения (min, min2, min4, …) пока не достигнет или не превысит max (степени двойки).
  - Формат min:max:step: указываете и начальное значение, и конечное, и шаг. Пример: 1:16:2 даст 1, 3, 5, 7, 9, 11, 13, 15.
- Когда менять:

  - Если вы хотите просто исследовать влияние в широком диапазоне — можно делать 1:32, чтобы охватить 1, 2, 4, 8, 16, 32 (степени двойки).
  - Если хотите более точный контроль, используйте явный шаг, например --sweep-range 10:50:10 (10, 20, 30, 40, 50).


## Когда какой sweep-type использовать
- batch_size — если сервер поддерживает батчинг и вы хотите узнать, есть ли прирост производительности или оптимальное значение размера батча. Важно для систем, где инференс может группироваться (например, Triton Server, TensorFlow Serving, PyTorch с DataLoader и т.п.).
- concurrency — самый популярный случай при тестировании нагрузок на модель: вы хотите узнать, как растёт latency и throughput, когда параллельно отправляются 1, 2, 4, 8, … запросов.
- request_rate — используется, если нужно задать фиксированную частоту запросов и смотреть, при каком RPS система начинает не справляться (SLA violation, увеличение задержки).
- input_sequence_length — когда критично понимать, как меняется производительность модели на разных размерах входа (для ChatGPT-подобных систем часто важно, сколько токенов в промпте).
- num_dataset_entries — когда есть разнообразный набор промптов, и вы хотите понять, влияет ли количество уникальных запросов на кэширование, re-use attention или что-то ещё.
Таким образом, выбор --sweep-type зависит от того, какой аспект производительности (или «нагрузки») вы хотите варьировать и анализировать.

## Бенчмарки различных параметров
В целом для основных замеров нам подойдут concurrency, request_rate и input_seqence_length

```shell
genai-perf analyze -m <model> --sweep-type concurrency --sweep-range 1:256
This will sweep over concurrencies of 1,2,4,8,16,32,64,128, and 256
```

```shell
genai-perf analyze -m <model> --sweep-type request_rate --sweep-range 100:500:50
This will sweep over request rate at values of 100,150,...450,500
```

```shell
genai-perf analyze -m <model> --sweep-type input_sequence_length --sweep-list 100,150,200,400
This will sweep over ISL for values of 100,150,200 and 400
```

## Как использовать
Например, проведем анализ для request_rate модели deepseek-r1-distill-qwen-1.5b
```shell
genai-perf analyze -m deepseek-r1-distill-qwen-1.5b --sweep-type request_rate --sweep-range 10:60:10
```

В результате получим следующий вывод:
```
Config Name,Request Rate,ISL,Num Dataset Entries,p99 Time To First Token (ms),p99 Inter Token Latency (ms),p99 Request Latency (ms),p99 Output Sequence Length (tokens),Avg. Output Token Throughput (tokens/sec),Request Throughput (requests/sec)
deepseek-r1-distill-qwen-1.5b_run_config_3,40,551,100,8846.92,0.00,8846.92,28.00,596.90,30.10
deepseek-r1-distill-qwen-1.5b_run_config_2,30,551,100,651.90,0.00,651.90,28.00,587.96,29.61
deepseek-r1-distill-qwen-1.5b_run_config_5,60,551,100,18346.84,0.00,18346.84,27.52,582.07,29.16
deepseek-r1-distill-qwen-1.5b_run_config_4,50,551,100,14925.60,0.00,14925.60,27.54,577.98,29.11
deepseek-r1-distill-qwen-1.5b_run_config_1,20,551,100,238.59,0.00,238.59,26.86,394.49,19.90
deepseek-r1-distill-qwen-1.5b_run_config_0,10,551,100,153.83,0.00,153.83,28.86,198.86,9.99

Config Name,GPU,p99 GPU Power Usage (W),p99 GPU Energy Consumption (MJ),p99 GPU Utilization (%),p99 GPU Memory Used (GB),Avg. GPU Power Limit (W),Avg. GPU Total Memory (GB)
deepseek-r1-distill-qwen-1.5b_run_config_3,gpu0,168.87,0.20,65.43,35.86,250.00,42.95
deepseek-r1-distill-qwen-1.5b_run_config_2,gpu0,169.46,0.19,65.90,35.86,250.00,42.95
deepseek-r1-distill-qwen-1.5b_run_config_5,gpu0,168.32,0.21,65.00,35.86,250.00,42.95
deepseek-r1-distill-qwen-1.5b_run_config_4,gpu0,168.32,0.20,65.00,35.86,250.00,42.95
deepseek-r1-distill-qwen-1.5b_run_config_1,gpu0,169.58,0.19,66.00,35.86,250.00,42.95
deepseek-r1-distill-qwen-1.5b_run_config_0,gpu0,169.58,0.18,66.00,35.86,250.00,42.95
```

Как видно из вывода, данный тест показывает метрики для каждого RPS, а также GPU утилизацию. Такой подход позволяет понять, как меняется производительность модели при изменении RPS и как меняется утилизация GPU при этом. Также по такой методике можно подбирать оптимальные параметры для конфигурации модели.