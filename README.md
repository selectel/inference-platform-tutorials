# inference-platform-tutorials
Tutorials for Selectel Inference Platform

Инференс-платформа Selectel — это облачное решение, которое позволяет быстро и эффективно развертывать и управлять API на основе собственных моделей машинного обучения. Платформа предлагает автоматическое масштабирование ресурсов и поддержку Open Source инструментов, таких как NVIDIA Triton™ Inference Server и Istio Ingress Controller. Это позволяет избежать привязки к конкретным вендорам и обеспечивает гибкость в настройке под различные нагрузки.

### Ключевые особенности:
- **Быстрое развертывание**: Позволяет запускать AI-проекты до 3 раз быстрее, сокращая время выхода на рынок.
- **Готовые API**: Возможность получения готового endpoint с ML-моделью за несколько минут.
- **Автоматическое масштабирование**: Платформа автоматически масштабирует ресурсы в зависимости от нагрузки.
- **Высокая производительность**: Использование выделенных GPU-ресурсов и интеграция с NVIDIA Triton™ Inference Server.
- **Круглосуточная поддержка**: Доступ к технической поддержке 24/7.

### Преимущества:
- **Гибкость и независимость**: Использование Open Source решений позволяет избежать vendor lock-in.
- **Экосистема продуктов**: Возможность интеграции с другими продуктами Selectel, такими как Kubernetes, объектное хранилище и CDN.

## Справка по Triton Inference Server
Ведение структуры репозитория в Triton Inference Server и работа с ним в Kubernetes (k8s) требует понимания нескольких ключевых аспектов. Вот основные правила и основы:

### Структура репозитория модели в Triton

1. **Репозиторий модели**: Это файловая система, где хранятся модели, которые Triton будет обслуживать. Репозиторий может находиться на локальной файловой системе или в облачном хранилище, таком как S3 или Google Cloud Storage.

2. **Структура папок**: Каждая модель должна находиться в своей директории внутри репозитория. Например:
   ```
   model_repository/
   ├── model1
   │   ├── 1
   │   │   └── model.onnx
   │   └── config.pbtxt
   └── model2
       ├── 1
       │   └── model.pt
       └── config.pbtxt
   ```
   Здесь 1 — это версия модели. Triton поддерживает версионирование моделей.

3. **Файл конфигурации (`config.pbtxt`)**: Этот файл определяет конфигурацию модели, включая информацию о входах и выходах, параметры пакетной обработки и другие настройки. Для некоторых бэкендов Triton может автоматически генерировать конфигурацию.

4. **Поддерживаемые форматы моделей**: Triton поддерживает множество форматов моделей, включая TensorRT, ONNX, PyTorch, TensorFlow и другие.

### Работа с Triton в Inference платформе

Этот репозиторий предназначен для обучения клиентов использованию инференс-платформы Selectel. В нем содержатся различные учебные материалы и примеры использования платформы.

- **Папка [`demo`](./demo-inferp)**: Содержит скрипты для запуска трех сценариев использования платформы:
  - Базовый деплой с авторизацией
  - Автоскейлинг
  - Канареечный деплой моделей

- **Папка [`LLM-and-Popular-models-tutorials`](./LLM-and-Popular-models-tutorials)**: Включает примеры деплоя популярных LLM (Large Language Models) и других моделей, таких как Whisper и Stable Diffusion, в нашу платформу.

- **Папка [`UI examples`](./UI-examples)**: Приведены примеры интерфейса к нашей платформе через Gradio и Telegram Bot.

- **Папка [`models format tutorial`](./models-format-tutorial)**: Содержит примеры деплоя различных форматов моделей.

- **Папка [`optimization`](./optimization)**: Содержит примеры оптимизации моделей с помощью Model Navigator и Model Analyzer.

Эта платформа идеально подходит для компаний, стремящихся быстро и эффективно внедрять AI-решения, минимизируя затраты на инфраструктуру и техническую поддержку.

### Полезные ссылки на документацию Triton

- [Начало работы с Triton](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html)
- [Архитектура Triton](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/architecture.html)
- [Руководство по репозиторию моделей](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html)
- [Оптимизация производительности](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/performance_tuning.html)
- [Часто задаваемые вопросы (FAQ)](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/faq.html)